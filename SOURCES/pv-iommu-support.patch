diff --git a/arch/x86/include/asm/xen/hypercall.h b/arch/x86/include/asm/xen/hypercall.h
index d66ba01478ac..1cef4d44a165 100644
--- a/arch/x86/include/asm/xen/hypercall.h
+++ b/arch/x86/include/asm/xen/hypercall.h
@@ -49,6 +49,7 @@
 #include <xen/interface/xen.h>
 #include <xen/interface/sched.h>
 #include <xen/interface/physdev.h>
+#include <xen/interface/pv-iommu.h>
 #include <xen/interface/platform.h>
 #include <xen/interface/xen-mca.h>
 
@@ -324,6 +325,12 @@ HYPERVISOR_multicall(void *call_list, uint32_t nr_calls)
 	return _hypercall2(int, multicall, call_list, nr_calls);
 }
 
+static inline int
+HYPERVISOR_iommu_op(void *uop, unsigned int count)
+{
+       return _hypercall2(int, iommu_op, uop, count);
+}
+
 static inline int
 HYPERVISOR_update_va_mapping(unsigned long va, pte_t new_val,
 			     unsigned long flags)
diff --git a/arch/x86/xen/pci-swiotlb-xen.c b/arch/x86/xen/pci-swiotlb-xen.c
index 37c6056a7bba..447d7bb733f0 100644
--- a/arch/x86/xen/pci-swiotlb-xen.c
+++ b/arch/x86/xen/pci-swiotlb-xen.c
@@ -2,21 +2,134 @@
 
 #include <linux/dma-mapping.h>
 #include <linux/pci.h>
+#include <linux/kthread.h>
 #include <xen/swiotlb-xen.h>
 
 #include <asm/xen/hypervisor.h>
 #include <xen/xen.h>
 #include <asm/iommu_table.h>
-
+#include <asm/xen/hypercall.h>
+#include <xen/interface/memory.h>
+#include <xen/hvc-console.h>
 
 #include <asm/xen/swiotlb-xen.h>
+#include <asm/xen/page.h>
 #ifdef CONFIG_X86_64
 #include <asm/iommu.h>
 #include <asm/dma.h>
 #endif
 #include <linux/export.h>
 
+#define IOMMU_BATCH_SIZE 128
+
+extern unsigned long max_pfn;
+dma_addr_t pv_iommu_1_to_1_offset;
+EXPORT_SYMBOL(pv_iommu_1_to_1_offset);
+
+bool pv_iommu_1_to_1_setup_complete;
+EXPORT_SYMBOL(pv_iommu_1_to_1_setup_complete);
+
 int xen_swiotlb __read_mostly;
+static struct pv_iommu_op iommu_ops[IOMMU_BATCH_SIZE];
+
+int xen_pv_iommu_map_sg_attrs(struct device *hwdev, struct scatterlist *sgl,
+			 int nelems, enum dma_data_direction dir,
+			 unsigned long attrs);
+
+dma_addr_t xen_pv_iommu_map_page(struct device *dev, struct page *page,
+				unsigned long offset, size_t size,
+				enum dma_data_direction dir,
+				unsigned long attrs);
+
+void *xen_pv_iommu_alloc_coherent(struct device *hwdev, size_t size,
+					dma_addr_t *dma_handle, gfp_t flags,
+					unsigned long attrs);
+
+void xen_pv_iommu_free_coherent(struct device *dev, size_t size,
+				      void *vaddr, dma_addr_t dma_addr,
+				      unsigned long attrs);
+
+static struct dma_map_ops xen_pv_iommu_dma_ops = {
+	.mapping_error = swiotlb_dma_mapping_error,
+	.alloc = swiotlb_alloc,
+	.free = swiotlb_free,
+	.map_sg = xen_pv_iommu_map_sg_attrs,
+	.map_page = xen_pv_iommu_map_page,
+	.unmap_sg = swiotlb_unmap_sg_attrs,
+	.unmap_page = swiotlb_unmap_page,
+	.get_required_mask = xen_swiotlb_get_required_mask,
+	.sync_single_for_cpu = swiotlb_sync_single_for_cpu,
+	.sync_single_for_device = swiotlb_sync_single_for_device,
+	.sync_sg_for_cpu = swiotlb_sync_sg_for_cpu,
+	.sync_sg_for_device = swiotlb_sync_sg_for_device,
+};
+
+int xen_iommu_map_page(unsigned long bfn, unsigned long mfn)
+{
+	struct pv_iommu_op iommu_op;
+	int rc;
+
+	iommu_op.u.map_page.bfn = bfn;
+	iommu_op.u.map_page.gfn = mfn;
+	iommu_op.flags = IOMMU_OP_readable | IOMMU_OP_writeable | IOMMU_MAP_OP_no_ref_cnt;
+	iommu_op.subop_id = IOMMUOP_map_page;
+	rc = HYPERVISOR_iommu_op(&iommu_op, 1);
+	if (rc < 0) {
+		printk("Failed to setup IOMMU mapping for gpfn 0x%lx, mfn 0x%lx, err %d\n",
+				bfn, mfn, rc);
+		return rc;
+	}
+	return iommu_op.status;
+}
+EXPORT_SYMBOL_GPL(xen_iommu_map_page);
+
+int xen_iommu_unmap_page(unsigned long bfn)
+{
+	struct pv_iommu_op iommu_op;
+	int rc;
+
+	iommu_op.u.unmap_page.bfn = bfn;
+	iommu_op.flags = IOMMU_MAP_OP_no_ref_cnt;
+	iommu_op.subop_id = IOMMUOP_unmap_page;
+	rc = HYPERVISOR_iommu_op(&iommu_op, 1);
+	if (rc < 0) {
+		printk("Failed to remove IOMMU mapping for gpfn 0x%lx, err %d\n", bfn, rc);
+		return rc;
+	}
+	return iommu_op.status;
+}
+
+int xen_iommu_batch(struct pv_iommu_op *iommu_ops, int count)
+{
+	int rc;
+
+	rc = HYPERVISOR_iommu_op(iommu_ops, count);
+	if (rc < 0) {
+		printk("Failed to batch IOMMU map, err %d\n", rc);
+	}
+	return rc;
+}
+EXPORT_SYMBOL_GPL(xen_iommu_batch);
+
+
+static int check_batch(int size)
+{
+	int op;
+	int res=0;
+	for (op = 1; op < size; op +=2)
+	{
+		if ( iommu_ops[op].status )
+		{
+			printk("Iommu op %d went wrong, subop id %d, bfn 0x%llx, gfn 0x%llx\n, err %d, flags 0x%x\n",
+			       op, iommu_ops[op].subop_id,
+			       iommu_ops[op].u.map_page.bfn,
+			       iommu_ops[op].u.map_page.gfn,
+			       iommu_ops[op].status, iommu_ops[op].flags );
+			res++;
+		}
+	}
+	return res;
+}
 
 /*
  * pci_xen_swiotlb_detect - set xen_swiotlb to 1 if necessary
@@ -26,10 +139,101 @@ int xen_swiotlb __read_mostly;
  */
 int __init pci_xen_swiotlb_detect(void)
 {
-
 	if (!xen_pv_domain())
 		return 0;
 
+	if (xen_initial_domain()){
+		int count = 0;
+		u64 pfn, pfn_limit, max_host_mfn = 0;
+		struct pv_iommu_op_ext iommu_op;
+		int rc;
+
+		iommu_op.u.query_caps.offset = 0;
+		iommu_op.flags = 0;
+		iommu_op.status = 0;
+		iommu_op.subop_id = IOMMUOP_query_caps;
+		rc = HYPERVISOR_iommu_op(&iommu_op, 1);
+
+		if (rc || !(iommu_op.flags & IOMMU_QUERY_map_cap))
+			goto no_pv_iommu;
+
+		max_host_mfn = HYPERVISOR_memory_op(XENMEM_maximum_ram_page, NULL);
+		printk("Max host RAM MFN is 0x%llx\n",max_host_mfn);
+		printk("max_pfn is 0x%lx\n",max_pfn);
+
+		/* Check and Setup 1-1 host RAM offset location */
+		if (iommu_op.flags & IOMMU_QUERY_map_all_mfns)
+			pv_iommu_1_to_1_offset = (dma_addr_t) iommu_op.u.query_caps.offset << PAGE_SHIFT;
+		/* If offset is 0 or not set - disable PV IOMMU */
+		if (!pv_iommu_1_to_1_offset)
+			goto no_pv_iommu;
+
+		pfn_limit = pv_iommu_1_to_1_offset >> PAGE_SHIFT;
+		if (max_pfn >= pfn_limit)
+		{
+			xen_raw_printk( "XEN-PV-IOMMU: bfn_foreign_offset"
+					" at %llu, is too small for Dom0."
+					"  Needs %lu\n", pfn_limit, max_pfn + 1);
+			goto remove_iommu_mappings;
+		}
+
+		pfn_limit = min(max_host_mfn, pfn_limit);
+
+		/* Setup 1-1 mapping of GPFN to MFN */
+		for (pfn=0; pfn < pfn_limit; pfn++)
+		{
+			unsigned long mfn = get_phys_to_machine(pfn);
+			if (mfn != INVALID_P2M_ENTRY && mfn != IDENTITY_FRAME(pfn))
+			{
+				iommu_ops[count].u.unmap_page.bfn = pfn;
+				iommu_ops[count].flags = IOMMU_MAP_OP_no_ref_cnt;
+				iommu_ops[count].subop_id = IOMMUOP_unmap_page;
+				count++;
+				iommu_ops[count].u.map_page.bfn = pfn;
+				iommu_ops[count].u.map_page.gfn = pfn_to_mfn(pfn);
+				iommu_ops[count].flags = IOMMU_OP_readable |
+							IOMMU_OP_writeable |
+							IOMMU_MAP_OP_no_ref_cnt;
+				iommu_ops[count].subop_id = IOMMUOP_map_page;
+				count++;
+			}
+			if (count == IOMMU_BATCH_SIZE)
+			{
+				count = 0;
+				if (xen_iommu_batch(iommu_ops,
+							IOMMU_BATCH_SIZE))
+					goto remove_iommu_mappings;
+
+				if (check_batch(IOMMU_BATCH_SIZE))
+				{
+					xen_raw_printk("Failed to fully Setup 1-1 mapping of GPFN to MFN.\n");
+					goto remove_iommu_mappings;
+				}
+			}
+
+		}
+		if (count) {
+			if (xen_iommu_batch(iommu_ops, count))
+				goto remove_iommu_mappings;
+			if (check_batch(count))
+			{
+				xen_raw_printk("Failed to fully Setup 1-1 mapping of GPFN to MFN.\n");
+				goto remove_iommu_mappings;
+			}
+		}
+
+		/* hook the PV IOMMU DMA ops */
+		xen_swiotlb = 0;
+
+		printk("Using GPFN IOMMU mode, 1-to-1 offset is 0x%llx\n",
+				pv_iommu_1_to_1_offset);
+		return 1;
+
+remove_iommu_mappings:
+		BUG();
+	}
+
+no_pv_iommu:
 	/* If running as PV guest, either iommu=soft, or swiotlb=force will
 	 * activate this IOMMU. If running as PV privileged, activate it
 	 * irregardless.
@@ -54,6 +258,15 @@ int __init pci_xen_swiotlb_detect(void)
 	return xen_swiotlb;
 }
 
+void __init pci_xen_pv_iommu_late_init(void)
+{
+	if (pv_iommu_1_to_1_offset){
+		/* Xen has already set up 1-1 mapping for us */
+		pv_iommu_1_to_1_setup_complete = true;
+		printk(KERN_INFO "XEN-PV-IOMMU - completed setting up 1-1 mapping\n");
+	}
+}
+
 void __init pci_xen_swiotlb_init(void)
 {
 	if (xen_swiotlb) {
@@ -65,6 +278,16 @@ void __init pci_xen_swiotlb_init(void)
 		pci_request_acs();
 #endif
 	}
+
+	/* Start the native swiotlb */
+	if (pv_iommu_1_to_1_offset) {
+		dma_ops = &xen_pv_iommu_dma_ops;
+		pci_request_acs();
+		swiotlb_init(0);
+		printk(KERN_INFO "XEN-PV-IOMMU: "
+		       "Using software bounce buffering for IO on 32bit DMA devices (SWIOTLB)\n");
+		swiotlb_print_info();
+	}
 }
 
 int pci_xen_swiotlb_init_late(void)
@@ -91,4 +314,4 @@ EXPORT_SYMBOL_GPL(pci_xen_swiotlb_init_late);
 IOMMU_INIT_FINISH(pci_xen_swiotlb_detect,
 		  NULL,
 		  pci_xen_swiotlb_init,
-		  NULL);
+		  pci_xen_pv_iommu_late_init);
diff --git a/arch/x86/xen/xen-head.S b/arch/x86/xen/xen-head.S
index 5077ead5e59c..4bd40dac7ca2 100644
--- a/arch/x86/xen/xen-head.S
+++ b/arch/x86/xen/xen-head.S
@@ -100,5 +100,6 @@ END(hypercall_page)
 	ELFNOTE(Xen, XEN_ELFNOTE_MOD_START_PFN,  .long 1)
 	ELFNOTE(Xen, XEN_ELFNOTE_HV_START_LOW,   _ASM_PTR __HYPERVISOR_VIRT_START)
 	ELFNOTE(Xen, XEN_ELFNOTE_PADDR_OFFSET,   _ASM_PTR 0)
+	ELFNOTE(XS, XS_ELFNOTE_PV_IOMMU, .long 1)
 
 #endif /*CONFIG_XEN */
diff --git a/drivers/xen/Makefile b/drivers/xen/Makefile
index 5d58370d81ac..6db17a6d3196 100644
--- a/drivers/xen/Makefile
+++ b/drivers/xen/Makefile
@@ -28,6 +28,7 @@ obj-$(CONFIG_XEN_SYS_HYPERVISOR)	+= sys-hypervisor.o
 obj-$(CONFIG_XEN_PVHVM)			+= platform-pci.o
 obj-$(CONFIG_XEN_TMEM)			+= tmem.o
 obj-$(CONFIG_SWIOTLB_XEN)		+= swiotlb-xen.o
+obj-$(CONFIG_SWIOTLB_XEN)		+= pv-iommu-xen.o
 obj-$(CONFIG_XEN_MCE_LOG)		+= mcelog.o
 obj-$(CONFIG_XEN_PCIDEV_BACKEND)	+= xen-pciback/
 obj-$(CONFIG_XEN_PRIVCMD)		+= xen-privcmd.o
diff --git a/drivers/xen/balloon.c b/drivers/xen/balloon.c
index d4e8b717ce2b..5879f7933977 100644
--- a/drivers/xen/balloon.c
+++ b/drivers/xen/balloon.c
@@ -113,6 +113,9 @@ static struct ctl_table xen_root[] = {
 
 #endif
 
+extern int xen_iommu_unmap_page(unsigned long pfn);
+extern dma_addr_t pv_iommu_1_to_1_offset;
+
 /*
  * Use one extent per PAGE_SIZE to avoid to break down the page into
  * multiple frame.
@@ -368,6 +371,15 @@ static enum bp_state reserve_additional_memory(void)
 
 static void xen_online_page(struct page *page)
 {
+#ifdef CONFIG_XEN_HAVE_PVMMU
+	/*
+	 * Clear any existing IOMMU mappings of the BFN (== PFN) for
+	 * this page, so the correct IOMMU mapping can be created when
+	 * the page is returned.
+	 */
+	if (pv_iommu_1_to_1_offset)
+		xen_iommu_unmap_page(page_to_pfn(page));
+#endif
 	__online_page_set_limits(page);
 
 	mutex_lock(&balloon_mutex);
diff --git a/drivers/xen/biomerge.c b/drivers/xen/biomerge.c
index 55ed80c3a17c..bdf57c63213f 100644
--- a/drivers/xen/biomerge.c
+++ b/drivers/xen/biomerge.c
@@ -3,13 +3,23 @@
 #include <linux/io.h>
 #include <linux/export.h>
 #include <xen/page.h>
+#include <xen/swiotlb-xen.h>
 
 bool xen_biovec_phys_mergeable(const struct bio_vec *vec1,
 			       const struct bio_vec *vec2)
 {
 #if XEN_PAGE_SIZE == PAGE_SIZE
-	unsigned long bfn1 = pfn_to_bfn(page_to_pfn(vec1->bv_page));
-	unsigned long bfn2 = pfn_to_bfn(page_to_pfn(vec2->bv_page));
+	unsigned long pfn1 = page_to_pfn(vec1->bv_page);
+	unsigned long pfn2 = page_to_pfn(vec2->bv_page);
+	unsigned long bfn1, bfn2;
+
+	if (!pv_iommu_1_to_1_offset) {
+		bfn1 = pfn_to_bfn(pfn1);
+		bfn2 = pfn_to_bfn(pfn2);
+	} else {
+		bfn1 = pfn1;
+		bfn2 = pfn2;
+	}
 
 	return bfn1 + PFN_DOWN(vec1->bv_offset + vec1->bv_len) == bfn2;
 #else
diff --git a/drivers/xen/ioemu.c b/drivers/xen/ioemu.c
index 7c89d11043b2..e9a8022d8e15 100644
--- a/drivers/xen/ioemu.c
+++ b/drivers/xen/ioemu.c
@@ -83,4 +83,49 @@ int xen_ioemu_inject_msi(domid_t domid, uint64_t addr, uint32_t data)
 
 	return HYPERVISOR_dm_op(domid, 1, &op_buf);
 }
+
+/**
+ * xen_ioemu_map_foreign_gfn_to_bfn: Returns the BFN's corresponding to GFN's.
+ * @pv_iommu_ops: pv_iommu_ops contains the struct_ map_foreign_page
+ * that will be used for lookup for BFN.
+ * @count: count of struct pv_iommu_ops.
+ *
+ * Its a wrapper function for getting BFN from GFN using IOMMU hypercall.
+*/
+int xen_ioemu_map_foreign_gfn_to_bfn(struct pv_iommu_op *ops, int count)
+{
+        int i;
+        int rc = 0;
+        for (i = 0; i < count; i++)
+        {
+                ops[i].subop_id = IOMMUOP_lookup_foreign_page;
+                ops[i].flags |= IOMMU_OP_writeable;
+        }
+        rc = HYPERVISOR_iommu_op(ops, count);
+        return rc;
+}
+
+/**
+ * xen_ioemu_unmap_foreign_gfn_to_bfn: Unmap BFN's corresponding to GFN's.
+ * @pv_iommu_ops: pv_iommu_ops contains the struct unmap_foreign_page
+ * that will be used to unmap BFNs.
+ * @count: count of struct pv_iommu_ops.
+ *
+ * Its a wrapper function to unmap foreign GFN's to BFN's .
+*/
+int xen_ioemu_unmap_foreign_gfn_to_bfn(struct pv_iommu_op *ops, int count)
+{
+        int i;
+        int rc = 0;
+        for (i = 0; i < count; i++)
+        {
+                ops[i].subop_id = IOMMUOP_unmap_foreign_page;
+        }
+        rc = HYPERVISOR_iommu_op(ops, count);
+        return rc;
+
+
+}
 EXPORT_SYMBOL(xen_ioemu_inject_msi);
+EXPORT_SYMBOL(xen_ioemu_map_foreign_gfn_to_bfn);
+EXPORT_SYMBOL(xen_ioemu_unmap_foreign_gfn_to_bfn);
diff --git a/drivers/xen/mem-reservation.c b/drivers/xen/mem-reservation.c
index 3782cf070338..1df1edeacfdb 100644
--- a/drivers/xen/mem-reservation.c
+++ b/drivers/xen/mem-reservation.c
@@ -16,6 +16,10 @@
 #include <xen/mem-reservation.h>
 #include <linux/moduleparam.h>
 
+extern int xen_iommu_map_page(unsigned long pfn, unsigned long mfn);
+extern int xen_iommu_unmap_page(unsigned long pfn);
+extern dma_addr_t pv_iommu_1_to_1_offset;
+
 bool __read_mostly xen_scrub_pages = IS_ENABLED(CONFIG_XEN_SCRUB_PAGES_DEFAULT);
 core_param(xen_scrub_pages, xen_scrub_pages, bool, 0);
 
@@ -35,6 +39,7 @@ void __xenmem_reservation_va_mapping_update(unsigned long count,
 	for (i = 0; i < count; i++) {
 		struct page *page = pages[i];
 		unsigned long pfn = page_to_pfn(page);
+		int ret;
 
 		BUG_ON(!page);
 
@@ -46,10 +51,24 @@ void __xenmem_reservation_va_mapping_update(unsigned long count,
 
 		set_phys_to_machine(pfn, frames[i]);
 
+		/*
+		 * Update the IOMMU mapping for this BFN (== PFN) now that we
+		 * have the new MFN.
+		 *
+		 * If this fails, leak the page as its not safe to use it (any
+		 * DMA will go somewhere unexpected causing memory corruption).
+		 */
+		if (pv_iommu_1_to_1_offset) {
+			ret = xen_iommu_map_page(pfn, frames[i]);
+			if (ret < 0) {
+				pr_err("leaking pfn %lx (iommu map failed: %d)\n",
+				       pfn, ret);
+				continue;
+			}
+		}
+
 		/* Link back into the page tables if not highmem. */
 		if (!PageHighMem(page)) {
-			int ret;
-
 			ret = HYPERVISOR_update_va_mapping(
 					(unsigned long)__va(pfn << PAGE_SHIFT),
 					mfn_pte(frames[i], PAGE_KERNEL),
@@ -84,6 +103,8 @@ void __xenmem_reservation_va_mapping_reset(unsigned long count,
 			BUG_ON(ret);
 		}
 		__set_phys_to_machine(pfn, INVALID_P2M_ENTRY);
+		if (pv_iommu_1_to_1_offset)
+			xen_iommu_unmap_page(pfn);
 	}
 }
 EXPORT_SYMBOL_GPL(__xenmem_reservation_va_mapping_reset);
diff --git a/drivers/xen/pv-iommu-xen.c b/drivers/xen/pv-iommu-xen.c
new file mode 100644
index 000000000000..23f84baa518f
--- /dev/null
+++ b/drivers/xen/pv-iommu-xen.c
@@ -0,0 +1,134 @@
+/*
+ *  Copyright 2014
+ *  by Malcolm Crossley <malcolm.crossley@citrix.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License v2.0 as published by
+ * the Free Software Foundation
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ *
+ */
+
+#include <linux/bootmem.h>
+#include <linux/dma-direct.h>
+#include <linux/export.h>
+#include <xen/swiotlb-xen.h>
+#include <asm/swiotlb.h>
+#include <xen/page.h>
+#include <xen/xen-ops.h>
+#include <xen/hvc-console.h>
+
+#include <trace/events/swiotlb.h>
+
+extern int xen_iommu_map_page(unsigned long pfn, unsigned long mfn);
+extern bool pv_iommu_1_to_1_setup_complete;
+
+extern phys_addr_t io_tlb_start, io_tlb_end;
+
+dma_addr_t xen_pv_iommu_get_foreign_addr(unsigned long p2m_entry)
+{
+	dma_addr_t phys;
+	unsigned long mfn = p2m_entry & ~FOREIGN_FRAME_BIT;
+	/* If 1-1 has not completed being setup then map this page now */
+	if (unlikely(!pv_iommu_1_to_1_setup_complete))
+		xen_iommu_map_page(mfn + (pv_iommu_1_to_1_offset >> PAGE_SHIFT),
+					mfn);
+
+	phys = (mfn << PAGE_SHIFT) + pv_iommu_1_to_1_offset;
+	return phys;
+}
+
+/*
+ * Map a single buffer of the indicated size for DMA in streaming mode.  The
+ * physical address to use is returned.
+ *
+ * PV IOMMU version detects Xen foreign pages and use's the original MFN offset
+ * into previously setup IOMMU 1-to-1 offset mapping of host memory
+ */
+dma_addr_t xen_pv_iommu_map_page(struct device *dev, struct page *page,
+				unsigned long offset, size_t size,
+				enum dma_data_direction dir,
+				unsigned long attrs)
+{
+	unsigned long p2m_entry = get_phys_to_machine(page_to_pfn(page));
+
+	if (p2m_entry & FOREIGN_FRAME_BIT) {
+		dma_addr_t phys = xen_pv_iommu_get_foreign_addr(p2m_entry) + offset;
+		/* Check if device can DMA to 1-1 mapped foreign address */
+		if (dma_capable(dev, phys, size)) {
+			return phys;
+		} else {
+			phys_addr_t map = swiotlb_tbl_map_single(dev, io_tlb_start,
+							page_to_phys(page) + offset,
+							size, dir, attrs);
+			trace_swiotlb_bounced(dev, phys, size, 0);
+			return phys_to_dma(dev, map);
+		}
+	}
+
+	return swiotlb_map_page(dev, page, offset, size, dir, attrs);
+}
+EXPORT_SYMBOL_GPL(xen_pv_iommu_map_page);
+
+/*
+ * Map a set of buffers described by scatterlist in streaming mode for DMA.
+ * This is the scatter-gather version of the above xen_swiotlb_map_page
+ * interface.  Here the scatter gather list elements are each tagged with the
+ * appropriate dma address and length.  They are obtained via
+ * sg_dma_{address,length}(SG).
+ *
+ * NOTE: An implementation may be able to use a smaller number of
+ *       DMA address/length pairs than there are SG table elements.
+ *       (for example via virtual mapping capabilities)
+ *       The routine returns the number of addr/length pairs actually
+ *       used, at most nents.
+ *
+ * PV IOMMU version detects Xen foreign pages and use's the original MFN offset
+ * into previously setup IOMMU 1-to-1 offset mapping of host memory
+ *
+ */
+int
+xen_pv_iommu_map_sg_attrs(struct device *hwdev, struct scatterlist *sgl,
+			 int nelems, enum dma_data_direction dir,
+			 unsigned long attrs)
+{
+	struct scatterlist *sg;
+	int i;
+
+	for_each_sg(sgl, sg, nelems, i) {
+		phys_addr_t paddr = sg_phys(sg);
+		dma_addr_t dev_addr = phys_to_dma(hwdev, paddr);
+		unsigned long p2m_entry = get_phys_to_machine(PFN_DOWN(paddr));
+		if (p2m_entry & FOREIGN_FRAME_BIT)
+			dev_addr = xen_pv_iommu_get_foreign_addr(p2m_entry) +
+					(paddr & ~PAGE_MASK);
+
+		/* Check if device can DMA to bus address */
+		if (!dma_capable(hwdev, dev_addr, sg->length)){
+			phys_addr_t map = swiotlb_tbl_map_single(hwdev, io_tlb_start,
+						paddr, sg->length, dir, attrs);
+			trace_swiotlb_bounced(hwdev, dev_addr, sg->length, 0);
+			if (map == SWIOTLB_MAP_ERROR) {
+				/* Don't panic here, we expect map_sg users
+				   to do proper error handling. */
+				swiotlb_unmap_sg_attrs(hwdev, sgl, i, dir,
+						       attrs);
+				sgl[0].dma_length = 0;
+				return 0;
+			}
+			sg->dma_address = phys_to_dma(hwdev, map);
+
+		} else {
+			sg->dma_address = dev_addr;
+		}
+		sg->dma_length = sg->length;
+	}
+	return nelems;
+
+}
+EXPORT_SYMBOL_GPL(xen_pv_iommu_map_sg_attrs);
diff --git a/include/xen/interface/elfnote.h b/include/xen/interface/elfnote.h
index 9e9f9bf7c66d..9f4ceba7cefe 100644
--- a/include/xen/interface/elfnote.h
+++ b/include/xen/interface/elfnote.h
@@ -207,6 +207,18 @@
  */
 #define XEN_ELFNOTE_MAX XEN_ELFNOTE_PHYS32_ENTRY
 
+
+/* XenServer specific ELF notes */
+
+/*
+ * If this note is present, Dom0 can use PV-IOMMU.
+ * Xen should initialise PV-IOMMU only if Dom0 supports it. Otherwise it leads
+ * to DMA errors.
+ */
+#define XS_ELFNOTE_PV_IOMMU               0
+
+#define XS_ELFNOTE_MAX XS_ELFNOTE_PV_IOMMU
+
 #endif /* __XEN_PUBLIC_ELFNOTE_H__ */
 
 /*
diff --git a/include/xen/interface/pv-iommu.h b/include/xen/interface/pv-iommu.h
new file mode 100644
index 000000000000..8b765445f229
--- /dev/null
+++ b/include/xen/interface/pv-iommu.h
@@ -0,0 +1,100 @@
+/*
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to
+ * deal in the Software without restriction, including without limitation the
+ * rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
+ * sell copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
+ * DEALINGS IN THE SOFTWARE.
+ */
+
+#ifndef __XEN_PUBLIC_PV_IOMMU_H__
+#define __XEN_PUBLIC_PV_IOMMU_H__
+
+#define IOMMUOP_query_caps            1
+#define IOMMUOP_map_page              2
+#define IOMMUOP_unmap_page            3
+#define IOMMUOP_map_foreign_page      4
+#define IOMMUOP_lookup_foreign_page   5
+#define IOMMUOP_unmap_foreign_page    6
+
+struct pv_iommu_op {
+    uint16_t subop_id;
+
+#define IOMMU_page_order (0xf1 << 10)
+#define IOMMU_get_page_order(flags) ((flags & IOMMU_page_order) >> 10)
+#define IOMMU_QUERY_map_cap (1 << 0)
+#define IOMMU_QUERY_map_all_mfns (1 << 1)
+#define IOMMU_OP_readable (1 << 0)
+#define IOMMU_OP_writeable (1 << 1)
+#define IOMMU_MAP_OP_no_ref_cnt (1 << 2)
+    uint16_t flags;
+    int32_t status;
+
+    union {
+        struct {
+            uint64_t bfn;
+            uint64_t gfn;
+        } map_page;
+
+        struct {
+            uint64_t bfn;
+        } unmap_page;
+
+        struct {
+            uint64_t bfn;
+            uint64_t gfn;
+            uint16_t domid;
+            uint16_t ioserver;
+        } map_foreign_page;
+
+        struct {
+            uint64_t bfn;
+            uint64_t gfn;
+            uint16_t domid;
+            uint16_t ioserver;
+        } lookup_foreign_page;
+
+        struct {
+            uint64_t bfn;
+            uint16_t ioserver;
+        } unmap_foreign_page;
+    } u;
+};
+
+struct pv_iommu_op_ext
+{
+    uint16_t subop_id;
+    uint16_t flags;
+    int32_t status;
+
+    union {
+        struct {
+            uint64_t offset;
+        } query_caps;
+    } u;
+};
+
+typedef struct pv_iommu_op pv_iommu_op_t;
+
+#endif
+
+/*
+ * Local variables:
+ * mode: C
+ * c-file-style: "BSD"
+ * c-basic-offset: 4
+ * tab-width: 4
+ * indent-tabs-mode: nil
+ * End:
+ */
diff --git a/include/xen/interface/xen.h b/include/xen/interface/xen.h
index 191696bbca5c..b0cdbb4b9f53 100644
--- a/include/xen/interface/xen.h
+++ b/include/xen/interface/xen.h
@@ -93,6 +93,7 @@
 #define __HYPERVISOR_arch_6               54
 #define __HYPERVISOR_arch_7               55
 
+#define __HYPERVISOR_iommu_op             56
 /*
  * VIRTUAL INTERRUPTS
  *
diff --git a/include/xen/ioemu.h b/include/xen/ioemu.h
index 95eb4e7c5060..fc44d7720b2d 100644
--- a/include/xen/ioemu.h
+++ b/include/xen/ioemu.h
@@ -29,7 +29,10 @@
  */
 #ifndef _XEN_IOEMU_H
 #define _XEN_IOEMU_H
+#include <xen/interface/pv-iommu.h>
 
 int xen_ioemu_inject_msi(domid_t domid, uint64_t addr, uint32_t data);
+int xen_ioemu_map_foreign_gfn_to_bfn(struct pv_iommu_op *ops, int count);
+int xen_ioemu_unmap_foreign_gfn_to_bfn(struct pv_iommu_op *ops, int count);
 
 #endif /* #ifndef _XEN_IOEMU_H */
diff --git a/include/xen/swiotlb-xen.h b/include/xen/swiotlb-xen.h
index ff66b40dfb53..5d39bf40e29b 100644
--- a/include/xen/swiotlb-xen.h
+++ b/include/xen/swiotlb-xen.h
@@ -10,4 +10,6 @@ extern const struct dma_map_ops xen_swiotlb_dma_ops;
 extern u64
 xen_swiotlb_get_required_mask(struct device *dev);
 
+extern dma_addr_t pv_iommu_1_to_1_offset;
+
 #endif /* __LINUX_SWIOTLB_XEN_H */
diff --git a/kernel/dma/swiotlb.c b/kernel/dma/swiotlb.c
index 4f8a6dbf0b60..530b0c326dc9 100644
--- a/kernel/dma/swiotlb.c
+++ b/kernel/dma/swiotlb.c
@@ -64,7 +64,10 @@ enum swiotlb_force swiotlb_force;
  * swiotlb_tbl_sync_single_*, to see if the memory was in fact allocated by this
  * API.
  */
-static phys_addr_t io_tlb_start, io_tlb_end;
+phys_addr_t io_tlb_start, io_tlb_end;
+
+EXPORT_SYMBOL_GPL(io_tlb_start);
+EXPORT_SYMBOL_GPL(io_tlb_end);
 
 /*
  * The number of IO TLB blocks (in groups of 64) between io_tlb_start and
diff --git a/kernel/ksysfs.c b/kernel/ksysfs.c
index 46ba853656f6..c0b63abc8672 100644
--- a/kernel/ksysfs.c
+++ b/kernel/ksysfs.c
@@ -184,6 +184,17 @@ static ssize_t rcu_normal_store(struct kobject *kobj,
 KERNEL_ATTR_RW(rcu_normal);
 #endif /* #ifndef CONFIG_TINY_RCU */
 
+extern dma_addr_t pv_iommu_1_to_1_offset;
+extern bool pv_iommu_1_to_1_setup_complete;
+
+static ssize_t pv_iommu_ready_show(struct kobject *kobj,
+				  struct kobj_attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", !pv_iommu_1_to_1_offset ||
+                       pv_iommu_1_to_1_setup_complete);
+}
+KERNEL_ATTR_RO(pv_iommu_ready);
+
 /*
  * Make /sys/kernel/notes give the raw contents of our kernel .notes section.
  */
@@ -231,6 +242,7 @@ static struct attribute * kernel_attrs[] = {
 	&rcu_expedited_attr.attr,
 	&rcu_normal_attr.attr,
 #endif
+	&pv_iommu_ready_attr.attr,
 	NULL
 };
 
